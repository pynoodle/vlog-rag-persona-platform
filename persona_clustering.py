# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import plotly.express as px
import plotly.graph_objects as go
import json
import os
import glob
from collections import Counter, defaultdict
from openai import OpenAI
import re

class PersonaClusterer:
    def __init__(self, data_path='youtube_data'):
        self.data_path = data_path
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # ë¼ì´í”„ìŠ¤íƒ€ì¼ ì•¡í‹°ë¹„í‹° í‚¤ì›Œë“œ ì •ì˜
        self.activity_keywords = {
            'cooking_eating': [
                'cook', 'cooking', 'recipe', 'food', 'eat', 'eating', 'meal', 'lunch', 'dinner', 'breakfast',
                'kitchen', 'baking', 'bake', 'snack', 'hungry', 'taste', 'delicious', 'yummy', 'ìš”ë¦¬', 'ë¨¹', 'ë°¥', 'ì‹ì‚¬'
            ],
            'reading_journaling': [
                'read', 'reading', 'book', 'books', 'journal', 'journaling', 'write', 'writing', 'study', 'studying',
                'learn', 'learning', 'note', 'notes', 'diary', 'ë…ì„œ', 'ì±…', 'ì¼ê¸°', 'ê³µë¶€'
            ],
            'shopping_haul': [
                'shop', 'shopping', 'haul', 'buy', 'buying', 'purchase', 'store', 'mall', 'online', 'amazon',
                'order', 'delivery', 'package', 'unboxing', 'ì‡¼í•‘', 'êµ¬ë§¤', 'í•˜ìš¸'
            ],
            'mindfulness_relaxation': [
                'relax', 'relaxing', 'meditation', 'meditate', 'mindful', 'mindfulness', 'calm', 'peaceful',
                'stress', 'anxiety', 'breath', 'breathing', 'yoga', 'zen', 'íœ´ì‹', 'ëª…ìƒ', 'ë§ˆìŒ'
            ],
            'exercise_fitness': [
                'exercise', 'workout', 'gym', 'fitness', 'run', 'running', 'walk', 'walking', 'dance', 'dancing',
                'stretch', 'stretching', 'cardio', 'strength', 'ìš´ë™', 'í—¬ìŠ¤', 'í”¼íŠ¸ë‹ˆìŠ¤'
            ],
            'beauty_skincare': [
                'beauty', 'skincare', 'makeup', 'make up', 'skin', 'face', 'routine', 'morning routine',
                'night routine', 'cleanser', 'moisturizer', 'serum', 'mask', 'ë·°í‹°', 'ìŠ¤í‚¨ì¼€ì–´', 'í™”ì¥'
            ],
            'travel_adventure': [
                'travel', 'trip', 'vacation', 'adventure', 'explore', 'exploring', 'journey', 'flight', 'plane',
                'hotel', 'airbnb', 'destination', 'ì—¬í–‰', 'íœ´ê°€', 'ì—¬í–‰'
            ],
            'art_craft': [
                'art', 'artistic', 'craft', 'crafting', 'draw', 'drawing', 'paint', 'painting', 'create',
                'creative', 'design', 'diy', 'project', 'ì˜ˆìˆ ', 'ê·¸ë¦¼', 'ë§Œë“¤ê¸°'
            ],
            'gaming': [
                'game', 'gaming', 'play', 'playing', 'video game', 'console', 'nintendo', 'playstation',
                'xbox', 'pc', 'mobile game', 'ê²Œì„', 'í”Œë ˆì´'
            ],
            'music': [
                'music', 'song', 'songs', 'sing', 'singing', 'dance', 'dancing', 'concert', 'album',
                'playlist', 'spotify', 'ìŒì•…', 'ë…¸ë˜', 'ì¶¤'
            ],
            'pet_care': [
                'pet', 'pets', 'dog', 'cat', 'puppy', 'kitten', 'animal', 'animals', 'care', 'walk',
                'feeding', 'feed', 'ë°˜ë ¤ë™ë¬¼', 'ê°•ì•„ì§€', 'ê³ ì–‘ì´'
            ],
            'home_decor': [
                'home', 'house', 'room', 'decor', 'decoration', 'furniture', 'interior', 'design',
                'cozy', 'aesthetic', 'vibe', 'í™ˆ', 'ì§‘', 'ì¸í…Œë¦¬ì–´', 'ê¾¸ë¯¸ê¸°'
            ],
            'tech_gadgets': [
                'phone', 'smartphone', 'iphone', 'android', 'tech', 'technology', 'gadget', 'device',
                'app', 'apps', 'digital', 'online', 'ìŠ¤ë§ˆíŠ¸í°', 'í°', 'ê¸°ìˆ '
            ],
            'fashion': [
                'fashion', 'style', 'outfit', 'clothes', 'clothing', 'dress', 'shirt', 'pants', 'shoes',
                'accessories', 'jewelry', 'íŒ¨ì…˜', 'ì˜·', 'ìŠ¤íƒ€ì¼'
            ],
            'photography': [
                'photo', 'photos', 'picture', 'pictures', 'camera', 'photography', 'photographer',
                'instagram', 'social media', 'ì‚¬ì§„', 'ì¹´ë©”ë¼'
            ],
            'social_events': [
                'party', 'celebration', 'event', 'meet', 'meeting', 'friend', 'friends', 'social',
                'hangout', 'gathering', 'íŒŒí‹°', 'ì¹œêµ¬', 'ëª¨ì„'
            ]
        }
    
    def load_channel_data(self):
        """ì±„ë„ í†µê³„ ë°ì´í„° ë¡œë“œ"""
        try:
            channels = pd.read_csv('channel_stats.csv')
            print(f"ì±„ë„ í†µê³„ ë¡œë“œ ì™„ë£Œ: {len(channels)}ê°œ ì±„ë„")
            return channels
        except FileNotFoundError:
            print("âŒ channel_stats.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
    def extract_channel_features(self, channel_id, channel_name):
        """íŠ¹ì • ì±„ë„ì˜ ë¼ì´í”„ìŠ¤íƒ€ì¼ íŠ¹ì§• ì¶”ì¶œ"""
        print(f"ğŸ“Š {channel_name} ì±„ë„ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        
        # ê¸°ë³¸ íŠ¹ì§• ì´ˆê¸°í™”
        features = {
            'channel_id': channel_id,
            'channel_name': channel_name,
            'total_videos': 0,
            'total_stt_files': 0,
            'total_views': 0,
            'avg_views': 0,
            'total_likes': 0,
            'avg_likes': 0,
            'total_comments': 0,
            'avg_comments': 0
        }
        
        # ë¼ì´í”„ìŠ¤íƒ€ì¼ ì•¡í‹°ë¹„í‹° íŠ¹ì§• ì´ˆê¸°í™”
        for activity in self.activity_keywords.keys():
            features[activity] = 0
        
        # ì±„ë„ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        channel_dir = os.path.join(self.data_path, channel_id)
        
        if not os.path.exists(channel_dir):
            print(f"âš ï¸ ì±„ë„ ë””ë ‰í† ë¦¬ ì—†ìŒ: {channel_dir}")
            return features
        
        # ì±„ë„ ì •ë³´ ë¡œë“œ
        channel_info_file = os.path.join(channel_dir, 'channel_info.json')
        if os.path.exists(channel_info_file):
            with open(channel_info_file, 'r', encoding='utf-8') as f:
                channel_info = json.load(f)
                features['total_videos'] = channel_info.get('total_videos_collected', 0)
        
        # STT íŒŒì¼ë“¤ ì²˜ë¦¬
        txt_files = glob.glob(os.path.join(channel_dir, "*.txt"))
        features['total_stt_files'] = len(txt_files)
        
        # ê° STT íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        total_content = ""
        for txt_file in txt_files:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read().lower()
                    total_content += content + " "
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {txt_file} - {e}")
                continue
        
        # ê° ì•¡í‹°ë¹„í‹°ë³„ í‚¤ì›Œë“œ ì¹´ìš´íŠ¸
        for activity, keywords in self.activity_keywords.items():
            count = 0
            for keyword in keywords:
                count += total_content.count(keyword.lower())
            features[activity] = count
        
        # ì±„ë„ í†µê³„ì—ì„œ ì¶”ê°€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        channels_df = self.load_channel_data()
        if channels_df is not None:
            channel_row = channels_df[channels_df['ì±„ë„ID'] == channel_id]
            if not channel_row.empty:
                features['total_views'] = int(channel_row['ì´ì¡°íšŒìˆ˜'].iloc[0])
                features['avg_views'] = int(channel_row['í‰ê· ì¡°íšŒìˆ˜'].iloc[0])
                features['total_likes'] = int(channel_row['ì´ì¢‹ì•„ìš”'].iloc[0])
                features['avg_likes'] = int(channel_row['í‰ê· ì¢‹ì•„ìš”'].iloc[0])
                features['total_comments'] = int(channel_row['ì´ëŒ“ê¸€'].iloc[0])
                features['avg_comments'] = int(channel_row['í‰ê· ëŒ“ê¸€'].iloc[0])
        
        return features
    
    def load_lifestyle_data(self):
        """ëª¨ë“  ì±„ë„ì˜ ë¼ì´í”„ìŠ¤íƒ€ì¼ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ”„ ë¼ì´í”„ìŠ¤íƒ€ì¼ ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        channels_df = self.load_channel_data()
        if channels_df is None:
            return None
        
        lifestyle_features = []
        
        for idx, row in channels_df.iterrows():
            channel_id = row['ì±„ë„ID']
            channel_name = row['ì±„ë„ëª…']
            
            features = self.extract_channel_features(channel_id, channel_name)
            lifestyle_features.append(features)
            
            if (idx + 1) % 10 == 0:
                print(f"ì§„í–‰ë¥ : {idx + 1}/{len(channels_df)} ì±„ë„ ì²˜ë¦¬ ì™„ë£Œ")
        
        df = pd.DataFrame(lifestyle_features)
        print(f"âœ… ë¼ì´í”„ìŠ¤íƒ€ì¼ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ì±„ë„")
        return df
    
    def perform_clustering(self, n_clusters=5):
        """K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        print(f"ğŸ” {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
        
        df = self.load_lifestyle_data()
        if df is None:
            return None, None, None
        
        # í´ëŸ¬ìŠ¤í„°ë§ìš© íŠ¹ì§• ì„ íƒ (ë¼ì´í”„ìŠ¤íƒ€ì¼ ì•¡í‹°ë¹„í‹°ë§Œ)
        activity_columns = list(self.activity_keywords.keys())
        X = df[activity_columns].copy()
        
        # ì •ê·œí™”
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        df['cluster'] = kmeans.fit_predict(X_scaled)
        
        # PCAë¡œ ì°¨ì› ì¶•ì†Œ (ì‹œê°í™”ìš©)
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_scaled)
        
        df['pca1'] = X_pca[:, 0]
        df['pca2'] = X_pca[:, 1]
        
        print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ì±„ë„ ìˆ˜:")
        for cluster_id in sorted(df['cluster'].unique()):
            count = len(df[df['cluster'] == cluster_id])
            print(f"   í´ëŸ¬ìŠ¤í„° {cluster_id}: {count}ê°œ ì±„ë„")
        
        return df, kmeans, scaler
    
    def analyze_cluster_characteristics(self, df):
        """ê° í´ëŸ¬ìŠ¤í„°ì˜ íŠ¹ì§• ë¶„ì„"""
        print("ğŸ“ˆ í´ëŸ¬ìŠ¤í„° íŠ¹ì§• ë¶„ì„ ì¤‘...")
        
        cluster_analysis = {}
        activity_columns = list(self.activity_keywords.keys())
        
        for cluster_id in sorted(df['cluster'].unique()):
            cluster_data = df[df['cluster'] == cluster_id]
            
            # í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ì•¡í‹°ë¹„í‹° ì ìˆ˜
            avg_activities = cluster_data[activity_columns].mean().sort_values(ascending=False)
            
            # ìƒìœ„ 5ê°œ ì•¡í‹°ë¹„í‹°
            top_activities = avg_activities.head(5)
            
            # í´ëŸ¬ìŠ¤í„° í†µê³„
            stats = {
                'cluster_id': cluster_id,
                'channel_count': len(cluster_data),
                'channels': cluster_data['channel_name'].tolist(),
                'avg_total_views': cluster_data['total_views'].mean(),
                'avg_avg_views': cluster_data['avg_views'].mean(),
                'avg_total_likes': cluster_data['total_likes'].mean(),
                'avg_avg_likes': cluster_data['avg_likes'].mean(),
                'top_activities': top_activities.to_dict(),
                'activity_scores': avg_activities.to_dict()
            }
            
            cluster_analysis[f'cluster_{cluster_id}'] = stats
        
        return cluster_analysis
    
    def generate_cluster_personas(self, cluster_analysis):
        """ê° í´ëŸ¬ìŠ¤í„°ì˜ í˜ë¥´ì†Œë‚˜ ìƒì„±"""
        print("ğŸ¤– í˜ë¥´ì†Œë‚˜ ìƒì„± ì¤‘...")
        
        personas = {}
        
        for cluster_key, cluster_data in cluster_analysis.items():
            cluster_id = cluster_data['cluster_id']
            top_activities = cluster_data['top_activities']
            channels = cluster_data['channels']
            
            # GPTë¡œ í˜ë¥´ì†Œë‚˜ ìƒì„±
            prompt = f"""
            ë‹¤ìŒì€ Gen Z ì¸í”Œë£¨ì–¸ì„œ í´ëŸ¬ìŠ¤í„°ì˜ ì£¼ìš” ë¼ì´í”„ìŠ¤íƒ€ì¼ í™œë™ì…ë‹ˆë‹¤:
            
            í´ëŸ¬ìŠ¤í„° {cluster_id}:
            - ì±„ë„ ìˆ˜: {cluster_data['channel_count']}ê°œ
            - ëŒ€í‘œ ì±„ë„ë“¤: {', '.join(channels[:5])}
            - ì£¼ìš” í™œë™ (ì ìˆ˜ ìˆœ):
            {chr(10).join([f"  - {activity}: {score:.1f}ì " for activity, score in top_activities.items()])}
            
            ì´ í´ëŸ¬ìŠ¤í„°ë¥¼ ëŒ€í‘œí•˜ëŠ” í˜ë¥´ì†Œë‚˜ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
            1. í˜ë¥´ì†Œë‚˜ ì´ë¦„ (ì°½ì˜ì ì´ê³  ê¸°ì–µí•˜ê¸° ì‰¬ìš´ ì´ë¦„)
            2. ë‚˜ì´ëŒ€ (Gen Z, 18-26ì„¸)
            3. ì„±ê²© íŠ¹ì§• (5ê°€ì§€)
            4. ì£¼ìš” ê´€ì‹¬ì‚¬ (5ê°€ì§€)
            5. ë§íˆ¬ íŠ¹ì§• (3ê°€ì§€)
            6. ëŒ€í‘œ ë¬¸êµ¬ (í˜ë¥´ì†Œë‚˜ê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” í‘œí˜„ 3ê°œ)
            7. ë¼ì´í”„ìŠ¤íƒ€ì¼ ì„¤ëª… (2-3ë¬¸ì¥)
            8. ì½˜í…ì¸  ìŠ¤íƒ€ì¼ (2-3ë¬¸ì¥)
            
            JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
            """
            
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.8
                )
                
                persona = json.loads(response.choices[0].message.content)
                persona['cluster_id'] = cluster_id
                persona['channels'] = channels
                persona['channel_count'] = cluster_data['channel_count']
                persona['top_activities'] = top_activities
                persona['stats'] = {
                    'avg_views': cluster_data['avg_avg_views'],
                    'avg_likes': cluster_data['avg_avg_likes']
                }
                
                personas[f'persona_{cluster_id}'] = persona
                
                print(f"âœ… í´ëŸ¬ìŠ¤í„° {cluster_id} í˜ë¥´ì†Œë‚˜ ìƒì„± ì™„ë£Œ: {persona.get('name', 'Unknown')}")
                
            except Exception as e:
                print(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id} í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹¤íŒ¨: {e}")
                continue
        
        return personas
    
    def create_visualization(self, df):
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”"""
        print("ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # PCA ì‹œê°í™”
        fig = px.scatter(
            df, 
            x='pca1', 
            y='pca2', 
            color='cluster',
            hover_data=['channel_name', 'total_views', 'avg_views'],
            title='Gen Z ì¸í”Œë£¨ì–¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼',
            labels={'pca1': 'PCA Component 1', 'pca2': 'PCA Component 2'}
        )
        
        fig.update_layout(
            width=800,
            height=600,
            showlegend=True
        )
        
        return fig
    
    def save_results(self, df, personas, cluster_analysis):
        """ê²°ê³¼ ì €ì¥"""
        print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ CSV ì €ì¥
        df.to_csv('persona_clusters.csv', index=False, encoding='utf-8-sig')
        print("âœ… persona_clusters.csv ì €ì¥ ì™„ë£Œ")
        
        # í˜ë¥´ì†Œë‚˜ ì •ë³´ JSON ì €ì¥
        with open('personas.json', 'w', encoding='utf-8') as f:
            json.dump(personas, f, ensure_ascii=False, indent=2)
        print("âœ… personas.json ì €ì¥ ì™„ë£Œ")
        
        # í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼ ì €ì¥
        with open('cluster_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(cluster_analysis, f, ensure_ascii=False, indent=2)
        print("âœ… cluster_analysis.json ì €ì¥ ì™„ë£Œ")
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_summary_report(personas, cluster_analysis)
    
    def generate_summary_report(self, personas, cluster_analysis):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("ğŸ“ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        report = []
        report.append("=" * 80)
        report.append("ğŸ­ Gen Z ì¸í”Œë£¨ì–¸ì„œ í˜ë¥´ì†Œë‚˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")
        report.append("=" * 80)
        report.append("")
        
        for persona_key, persona in personas.items():
            cluster_id = persona['cluster_id']
            report.append(f"ğŸ¯ í˜ë¥´ì†Œë‚˜ {cluster_id}: {persona.get('name', 'Unknown')}")
            report.append("-" * 60)
            report.append(f"ğŸ“Š ì±„ë„ ìˆ˜: {persona['channel_count']}ê°œ")
            report.append(f"ğŸ‘¥ ëŒ€í‘œ ì±„ë„: {', '.join(persona['channels'][:5])}")
            report.append(f"ğŸ“ˆ í‰ê·  ì¡°íšŒìˆ˜: {persona['stats']['avg_views']:,.0f}")
            report.append(f"â¤ï¸ í‰ê·  ì¢‹ì•„ìš”: {persona['stats']['avg_likes']:,.0f}")
            report.append("")
            
            if 'personality' in persona:
                report.append("ğŸ§  ì„±ê²© íŠ¹ì§•:")
                for trait in persona['personality']:
                    report.append(f"  - {trait}")
                report.append("")
            
            if 'interests' in persona:
                report.append("ğŸ¯ ì£¼ìš” ê´€ì‹¬ì‚¬:")
                for interest in persona['interests']:
                    report.append(f"  - {interest}")
                report.append("")
            
            if 'speaking_style' in persona:
                report.append("ğŸ’¬ ë§íˆ¬ íŠ¹ì§•:")
                for style in persona['speaking_style']:
                    report.append(f"  - {style}")
                report.append("")
            
            if 'catchphrases' in persona:
                report.append("ğŸ”¥ ëŒ€í‘œ ë¬¸êµ¬:")
                for phrase in persona['catchphrases']:
                    report.append(f"  - \"{phrase}\"")
                report.append("")
            
            if 'lifestyle' in persona:
                report.append("ğŸ  ë¼ì´í”„ìŠ¤íƒ€ì¼:")
                report.append(f"  {persona['lifestyle']}")
                report.append("")
            
            if 'content_style' in persona:
                report.append("ğŸ¬ ì½˜í…ì¸  ìŠ¤íƒ€ì¼:")
                report.append(f"  {persona['content_style']}")
                report.append("")
            
            report.append("=" * 80)
            report.append("")
        
        # íŒŒì¼ë¡œ ì €ì¥
        with open('persona_summary_report.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        
        print("âœ… persona_summary_report.txt ì €ì¥ ì™„ë£Œ")
    
    def run_full_analysis(self, n_clusters=5):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("Gen Z ì¸í”Œë£¨ì–¸ì„œ í˜ë¥´ì†Œë‚˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘!")
        print("=" * 60)
        
        # 1. í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        df, kmeans, scaler = self.perform_clustering(n_clusters)
        if df is None:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨")
            return None
        
        # 2. í´ëŸ¬ìŠ¤í„° íŠ¹ì§• ë¶„ì„
        cluster_analysis = self.analyze_cluster_characteristics(df)
        
        # 3. í˜ë¥´ì†Œë‚˜ ìƒì„±
        personas = self.generate_cluster_personas(cluster_analysis)
        
        # 4. ì‹œê°í™” ìƒì„±
        fig = self.create_visualization(df)
        fig.write_html('persona_clustering_visualization.html')
        print("âœ… persona_clustering_visualization.html ì €ì¥ ì™„ë£Œ")
        
        # 5. ê²°ê³¼ ì €ì¥
        self.save_results(df, personas, cluster_analysis)
        
        print("\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
        print("ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        print("  - persona_clusters.csv: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")
        print("  - personas.json: í˜ë¥´ì†Œë‚˜ ì •ë³´")
        print("  - cluster_analysis.json: í´ëŸ¬ìŠ¤í„° ë¶„ì„")
        print("  - persona_summary_report.txt: ìš”ì•½ ë¦¬í¬íŠ¸")
        print("  - persona_clustering_visualization.html: ì‹œê°í™”")
        
        return df, personas, cluster_analysis

# ì‹¤í–‰
if __name__ == "__main__":
    # OpenAI API í‚¤ í™•ì¸
    if not os.getenv('OPENAI_API_KEY'):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        print("   export OPENAI_API_KEY=your-api-key-here")
        exit(1)
    
    # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
    clusterer = PersonaClusterer()
    results = clusterer.run_full_analysis(n_clusters=5)
    
    if results:
        df, personas, cluster_analysis = results
        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"  - ì´ {len(df)}ê°œ ì±„ë„ ë¶„ì„")
        print(f"  - {len(personas)}ê°œ í˜ë¥´ì†Œë‚˜ ìƒì„±")
        print(f"  - {len(cluster_analysis)}ê°œ í´ëŸ¬ìŠ¤í„° ë¶„ì„")
