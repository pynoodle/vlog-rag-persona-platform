# -*- coding: utf-8 -*-
"""
ì „ì²´ STT ë°ì´í„° í† í”½ ë¶„ì„
ëª¨ë“  ìˆ˜ì§‘ëœ ì˜ìƒì˜ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” í† í”½ ì¶”ì¶œ
"""

import json
import os
import glob
from collections import Counter, defaultdict
import re
import sys

# Windows ì½˜ì†” UTF-8 ì„¤ì •
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

def load_all_videos():
    """ëª¨ë“  ì±„ë„ì˜ ì˜ìƒ ë°ì´í„° ë¡œë“œ"""
    all_videos = []
    
    for json_file in glob.glob('youtube_data/*/*.json'):
        if 'channel_info' in json_file:
            continue
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                video = json.load(f)
                
                # ì±„ë„ ì •ë³´ ì¶”ê°€
                channel_id = os.path.basename(os.path.dirname(json_file))
                video['channel_id'] = channel_id
                
                # ì „ì²´ í…ìŠ¤íŠ¸ ìƒì„±
                full_text = ""
                if video.get('transcript'):
                    full_text = ' '.join([item['text'] for item in video['transcript']])
                elif video.get('stt_text'):
                    full_text = video['stt_text']
                
                video['full_text'] = full_text
                video['has_text'] = bool(full_text.strip())
                
                all_videos.append(video)
        except Exception as e:
            print(f"Error loading {json_file}: {e}")
    
    return all_videos

def extract_keywords(text, min_length=3):
    """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)"""
    # ì†Œë¬¸ì ë³€í™˜ ë° ë‹¨ì–´ ë¶„ë¦¬
    words = re.findall(r'\b\w+\b', text.lower())
    
    # ë¶ˆìš©ì–´ ì œê±°
    stopwords = {
        'the', 'is', 'at', 'which', 'on', 'and', 'a', 'an', 'as', 'are', 'was', 'were',
        'to', 'in', 'for', 'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through',
        'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',
        'my', 'your', 'his', 'her', 'its', 'our', 'their', 'this', 'that', 'these', 'those',
        'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should', 'could',
        'be', 'been', 'being', 'am', 'so', 'just', 'like', 'know', 'get', 'go', 'going',
        'really', 'very', 'much', 'can', 'but', 'or', 'if', 'because', 'when', 'where',
        'what', 'who', 'how', 'all', 'there', 'some', 'out', 'than', 'other', 'now',
        'make', 'made', 'want', 'see', 'look', 'use', 'think', 'also', 'back', 'way',
        'even', 'well', 'need', 'thing', 'things', 'time', 'got', 'gonna', 'yeah', 'okay',
        'ê°€', 'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë”', 'ë§¤ìš°', 'ì •ë§', 'ì§„ì§œ', 'ì¢€',
        'í•œ', 'ë˜', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ë˜ì„œ', 'ì™œëƒí•˜ë©´', 'ë•Œë¬¸ì—', 'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤'
    }
    
    # ìµœì†Œ ê¸¸ì´ ì´ìƒ, ë¶ˆìš©ì–´ ì œì™¸
    filtered_words = [w for w in words if len(w) >= min_length and w not in stopwords]
    
    return filtered_words

def categorize_topics(videos):
    """í† í”½ë³„ë¡œ ì˜ìƒ ë¶„ë¥˜"""
    
    # ì£¼ìš” í† í”½ í‚¤ì›Œë“œ ì •ì˜
    topic_keywords = {
        'ğŸ¨ ë·°í‹° & ë©”ì´í¬ì—…': [
            'makeup', 'beauty', 'skincare', 'hair', 'nail', 'lipstick', 'foundation', 
            'mascara', 'eyeshadow', 'blush', 'concealer', 'primer', 'serum', 'moisturizer',
            'ë©”ì´í¬ì—…', 'í™”ì¥', 'ë·°í‹°', 'ìŠ¤í‚¨ì¼€ì–´', 'í—¤ì–´', 'ë„¤ì¼', 'ë¦½ìŠ¤í‹±'
        ],
        'ğŸ‘— íŒ¨ì…˜ & ì‡¼í•‘': [
            'outfit', 'fashion', 'clothes', 'shopping', 'haul', 'dress', 'style', 'wear',
            'jeans', 'shirt', 'shoes', 'bag', 'accessory', 'jewelry', 'wardrobe',
            'ì˜·', 'íŒ¨ì…˜', 'ì‡¼í•‘', 'ìŠ¤íƒ€ì¼', 'ê°€ë°©', 'ì‹ ë°œ', 'ì•¡ì„¸ì„œë¦¬'
        ],
        'ğŸ³ ìŒì‹ & ìš”ë¦¬': [
            'food', 'cook', 'recipe', 'eat', 'meal', 'dinner', 'lunch', 'breakfast',
            'kitchen', 'coffee', 'drink', 'restaurant', 'dessert', 'ingredient',
            'ìŒì‹', 'ìš”ë¦¬', 'ë ˆì‹œí”¼', 'ë¨¹', 'ì‹ì‚¬', 'ì»¤í”¼', 'ì¹´í˜'
        ],
        'ğŸ’ª ê±´ê°• & í”¼íŠ¸ë‹ˆìŠ¤': [
            'workout', 'fitness', 'exercise', 'gym', 'health', 'diet', 'weight', 'yoga',
            'cardio', 'muscle', 'training', 'routine', 'body',
            'ìš´ë™', 'í—¬ìŠ¤', 'ë‹¤ì´ì–´íŠ¸', 'ê±´ê°•', 'ìš”ê°€', 'í”¼íŠ¸ë‹ˆìŠ¤'
        ],
        'ğŸ“š í•™êµ & ê³µë¶€': [
            'school', 'study', 'student', 'class', 'exam', 'homework', 'notebook', 'pencil',
            'college', 'university', 'grade', 'test', 'learn', 'education',
            'í•™êµ', 'ê³µë¶€', 'ì‹œí—˜', 'ìˆ˜ì—…', 'ë…¸íŠ¸', 'í•„ê¸°', 'ëŒ€í•™'
        ],
        'ğŸ  ì¼ìƒ & ë¼ì´í”„ìŠ¤íƒ€ì¼': [
            'vlog', 'day', 'morning', 'routine', 'life', 'home', 'room', 'organize',
            'clean', 'decoration', 'apartment', 'house', 'bedroom', 'daily',
            'ì¼ìƒ', 'ë¸Œì´ë¡œê·¸', 'ë£¨í‹´', 'ì§‘', 'ë°©', 'ì •ë¦¬'
        ],
        'âœˆï¸ ì—¬í–‰ & íœ´ê°€': [
            'travel', 'trip', 'vacation', 'hotel', 'flight', 'airport', 'tour', 'visit',
            'beach', 'city', 'country', 'destination', 'adventure',
            'ì—¬í–‰', 'íœ´ê°€', 'í˜¸í…”', 'ë¹„í–‰ê¸°', 'ê³µí•­'
        ],
        'ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ & ì°½ì—…': [
            'business', 'work', 'office', 'entrepreneur', 'launch', 'product', 'brand',
            'marketing', 'sales', 'company', 'startup', 'client', 'meeting',
            'ë¹„ì¦ˆë‹ˆìŠ¤', 'ì‚¬ì—…', 'íšŒì‚¬', 'ë¸Œëœë“œ', 'ì œí’ˆ', 'ëŸ°ì¹­'
        ],
        'ğŸ“± ìŠ¤ë§ˆíŠ¸í° & IT': [
            'smartphone', 'iphone', 'android', 'phone', 'app', 'camera', 'video', 'photo',
            'battery', 'charger', 'case', 'screen', 'tablet', 'ipad', 'laptop', 'computer',
            'ìŠ¤ë§ˆíŠ¸í°', 'ì•„ì´í°', 'í°', 'ì•±', 'ì¹´ë©”ë¼', 'ë°°í„°ë¦¬', 'ì¼€ì´ìŠ¤'
        ],
        'ğŸ¥ ì½˜í…ì¸  ì œì‘': [
            'film', 'filming', 'camera', 'edit', 'editing', 'youtube', 'content', 'creator',
            'vlog', 'thumbnail', 'upload', 'video', 'shoot', 'lighting',
            'ì´¬ì˜', 'í¸ì§‘', 'ì˜ìƒ', 'ìœ íŠœë¸Œ', 'ì½˜í…ì¸ ', 'í¬ë¦¬ì—ì´í„°'
        ],
        'ğŸ’‘ ê´€ê³„ & ì¹œêµ¬': [
            'friend', 'boyfriend', 'girlfriend', 'relationship', 'date', 'family', 'mom',
            'dad', 'sister', 'brother', 'partner', 'love', 'friendship',
            'ì¹œêµ¬', 'ë‚¨ì¹œ', 'ì—¬ì¹œ', 'ê°€ì¡±', 'ì—„ë§ˆ', 'ì•„ë¹ ', 'ì‚¬ë‘'
        ],
        'ğŸŒ± í™˜ê²½ & ì‚¬íšŒ': [
            'climate', 'environment', 'sustainability', 'activism', 'change', 'future',
            'planet', 'carbon', 'emissions', 'crisis', 'protest',
            'í™˜ê²½', 'ê¸°í›„', 'ì§€ì†ê°€ëŠ¥', 'ë³€í™”', 'ë¯¸ë˜'
        ],
        'ğŸ¬ ì—”í„°í…Œì¸ë¨¼íŠ¸': [
            'movie', 'music', 'show', 'concert', 'festival', 'celebrity', 'entertainment',
            'song', 'album', 'performance', 'event', 'party',
            'ì˜í™”', 'ìŒì•…', 'ê³µì—°', 'í˜ìŠ¤í‹°ë²Œ', 'íŒŒí‹°'
        ]
    }
    
    # í† í”½ë³„ ì˜ìƒ ë¶„ë¥˜
    topic_videos = defaultdict(list)
    topic_keyword_counts = defaultdict(Counter)
    
    for video in videos:
        if not video.get('has_text'):
            continue
        
        text = video['full_text'].lower()
        video_topics = []
        
        for topic, keywords in topic_keywords.items():
            # í‚¤ì›Œë“œ ë§¤ì¹­
            matched_keywords = [kw for kw in keywords if kw.lower() in text]
            
            if matched_keywords:
                video_topics.append(topic)
                topic_videos[topic].append(video)
                
                # í‚¤ì›Œë“œë³„ ì¹´ìš´íŠ¸
                for kw in matched_keywords:
                    count = text.count(kw.lower())
                    topic_keyword_counts[topic][kw] += count
        
        video['topics'] = video_topics
    
    return topic_videos, topic_keyword_counts

def analyze_word_frequency(videos, top_n=100):
    """ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„"""
    all_text = " ".join([v['full_text'] for v in videos if v.get('has_text')])
    
    words = extract_keywords(all_text)
    word_freq = Counter(words)
    
    return word_freq.most_common(top_n)

def main():
    print("="*80)
    print("ğŸ“Š ì „ì²´ STT ë°ì´í„° í† í”½ ë¶„ì„")
    print("="*80)
    print()
    
    # ë°ì´í„° ë¡œë“œ
    print("ë°ì´í„° ë¡œë”© ì¤‘...")
    videos = load_all_videos()
    text_videos = [v for v in videos if v.get('has_text')]
    
    print(f"âœ“ ì´ {len(videos)}ê°œ ì˜ìƒ ë¡œë“œ")
    print(f"âœ“ í…ìŠ¤íŠ¸ ìˆëŠ” ì˜ìƒ: {len(text_videos)}ê°œ")
    print()
    
    # í† í”½ ë¶„ë¥˜
    print("í† í”½ ë¶„ë¥˜ ì¤‘...")
    topic_videos, topic_keyword_counts = categorize_topics(videos)
    
    # ê²°ê³¼ ì €ì¥í•  ë¬¸ìì—´
    output = []
    output.append("="*80)
    output.append("ğŸ“Š ì „ì²´ STT ë°ì´í„° í† í”½ ë¶„ì„ ê²°ê³¼")
    output.append("="*80)
    output.append("")
    output.append(f"ì´ ì˜ìƒ: {len(videos)}ê°œ")
    output.append(f"í…ìŠ¤íŠ¸ ìˆìŒ: {len(text_videos)}ê°œ")
    output.append(f"ë¶„ì„ ì±„ë„: {len(set(v['channel_id'] for v in videos))}ê°œ")
    output.append("")
    
    # í† í”½ë³„ ì •ë ¬ (ì˜ìƒ ìˆ˜ ê¸°ì¤€)
    sorted_topics = sorted(topic_videos.items(), key=lambda x: len(x[1]), reverse=True)
    
    output.append("="*80)
    output.append("ğŸ“‹ í† í”½ë³„ ì˜ìƒ ìˆ˜ ìš”ì•½")
    output.append("="*80)
    output.append("")
    
    for topic, vids in sorted_topics:
        output.append(f"{topic}: {len(vids)}ê°œ ì˜ìƒ")
    
    output.append("")
    
    # ê° í† í”½ ìƒì„¸ ë¶„ì„
    for topic, vids in sorted_topics:
        output.append("")
        output.append("="*80)
        output.append(f"{topic} ({len(vids)}ê°œ ì˜ìƒ)")
        output.append("="*80)
        output.append("")
        
        # ì£¼ìš” í‚¤ì›Œë“œ (ë¹ˆë„ìˆœ)
        keyword_counts = topic_keyword_counts[topic]
        top_keywords = keyword_counts.most_common(20)
        
        output.append("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ (ì–¸ê¸‰ íšŸìˆ˜):")
        for kw, count in top_keywords:
            output.append(f"  - {kw}: {count}íšŒ")
        output.append("")
        
        # ì±„ë„ë³„ ì˜ìƒ ìˆ˜
        channel_counts = Counter([v['metadata']['channel_title'] for v in vids])
        output.append("ğŸ“º ì±„ë„ë³„ ë¶„í¬:")
        for channel, count in channel_counts.most_common(10):
            output.append(f"  - {channel}: {count}ê°œ")
        output.append("")
        
        # ëŒ€í‘œ ì˜ìƒ (ì¡°íšŒìˆ˜ ë†’ì€ ìˆœ)
        sorted_vids = sorted(vids, key=lambda x: x['metadata'].get('view_count', '0').replace(',', ''), reverse=True)
        
        output.append("â­ ì¸ê¸° ì˜ìƒ TOP 5:")
        for idx, video in enumerate(sorted_vids[:5], 1):
            meta = video['metadata']
            output.append(f"  [{idx}] {meta['title']}")
            output.append(f"      ì±„ë„: {meta['channel_title']}")
            output.append(f"      ì¡°íšŒìˆ˜: {meta.get('view_count', 'N/A')}")
            output.append(f"      URL: {meta.get('video_url', 'N/A')}")
            output.append("")
    
    # ì „ì²´ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„
    output.append("")
    output.append("="*80)
    output.append("ğŸ“ˆ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (TOP 100)")
    output.append("="*80)
    output.append("")
    
    word_freq = analyze_word_frequency(videos, top_n=100)
    
    for idx, (word, count) in enumerate(word_freq, 1):
        output.append(f"{idx:3d}. {word}: {count:,}íšŒ")
    
    output.append("")
    
    # íŒŒì¼ë¡œ ì €ì¥
    output_text = '\n'.join(output)
    
    with open('all_topics_analysis.txt', 'w', encoding='utf-8') as f:
        f.write(output_text)
    
    # í™”ë©´ì—ë„ ì¶œë ¥
    print(output_text)
    
    print()
    print("="*80)
    print("âœ“ ë¶„ì„ ì™„ë£Œ!")
    print("âœ“ ê²°ê³¼ ì €ì¥: all_topics_analysis.txt")
    print("="*80)

if __name__ == "__main__":
    main()

